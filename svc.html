<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="utf-8">
  <link href="webrtc.css" rel="stylesheet">
  <title>Scalable Video Coding (SVC) Extension for WebRTC</title>
  <script class="remove" src="respec-w3c-common.js" type="text/javascript"></script>
  <script src="svc-respec-config.js" class="remove"></script>
</head>
<body>
  <section id="abstract">
    <p>This document defines a set of ECMAScript APIs in WebIDL to extend the WebRTC 1.0 API
    to enable user agents to support scalable video coding (SVC).</p>
  </section>
  <section id="sotd">
    <p>The API is based on preliminary work done in the W3C ORTC Community Group.</p>
  </section>
  <section class="informative" id="intro">
    <h2>Introduction</h2>
    <p>This specification extends the WebRTC specification [[!WEBRTC]] to
    enable user agents to support scalable video coding (SVC).</p>
  </section>
  <section id="conformance">
    <p>This specification defines conformance criteria that apply to a single
    product: the <dfn>user agent</dfn> that implements the interfaces that it
    contains.</p>
    <p>Conformance requirements phrased as algorithms or specific steps may be
    implemented in any manner, so long as the end result is equivalent. (In
    particular, the algorithms defined in this specification are intended to be
    easy to follow, and not intended to be performant.)</p>
    <p>Implementations that use ECMAScript to implement the APIs defined in
    this specification MUST implement them in a manner consistent with the
    ECMAScript Bindings defined in the Web IDL specification [[!WEBIDL-1]], as
    this specification uses that specification and terminology.</p>
  </section>
  <section>
    <h2>Terminology</h2>
     <p>The <code><a href=
      "http://dev.w3.org/html5/spec/webappapis.html#eventhandler">EventHandler</a></code>
      interface, representing a callback used for event handlers, and the <a href=
      "http://dev.w3.org/html5/spec/webappapis.html#errorevent"><code><dfn>ErrorEvent</dfn></code></a>
      interface are defined in [[!HTML51]].</p>
      <p>The concepts <dfn><a href=
      "http://dev.w3.org/html5/spec/webappapis.html#queue-a-task">queue a task</a></dfn>,
      <dfn><a href=
      "http://dev.w3.org/html5/spec/webappapis.html#fire-a-simple-event">fires a simple
      event</a></dfn> and <dfn><a href=
      "http://dev.w3.org/html5/spec/webappapis.html#networking-task-source">networking
      task source</a></dfn> are defined in [[!HTML51]].</p>
      <p>The terms <dfn>event</dfn>, <dfn><a href=
      "http://dev.w3.org/html5/spec/webappapis.html#event-handlers">event
      handlers</a></dfn> and <dfn><a href=
      "http://dev.w3.org/html5/spec/webappapis.html#event-handler-event-type">event
      handler event types</a></dfn> are defined in [[!HTML51]].</p>
      <p>When referring to exceptions, the terms <dfn><a
      href="https://www.w3.org/TR/WebIDL-1/#dfn-throw">throw</a></dfn> and
      <dfn data-dfn-for="exception"><a href=
      "https://www.w3.org/TR/WebIDL-1/#dfn-create-exception">create</a></dfn> are
      defined in [[!WEBIDL-1]].</p>
      <p>The terms <dfn data-lt="fulfill|fulfillment">fulfilled</dfn>, <dfn
      data-lt="reject|rejection|rejecting">rejected</dfn>,
      <dfn data-lt="resolve|resolves">resolved</dfn>, <dfn>pending</dfn> and
      <dfn>settled</dfn> used in the context of Promises are defined in
      [[!ECMASCRIPT-6.0]].</p>
      <p>The terms <dfn>MediaStream</dfn>, <dfn>MediaStreamTrack</dfn>, and
      <dfn>MediaStreamConstraints</dfn> are defined in [[!GETUSERMEDIA]].</p>
      <p>This specification references objects, internal slots and dictionaries defined in
      [[!WEBRTC]], including the <dfn>RTCPeerConnection</dfn> object (defined in Section 4.4),
      the <dfn>RTCRtpParameters</dfn> dictionary (defined in Section 5.2.1),
      the <dfn>RTCRtpSendParameters</dfn> dictionary (defined in Section 5.2.2),
      the <dfn>RTCRtpReceiveParameters</dfn> dictionary (defined in Section 5.2.3),
      the <dfn>RTCRtpCodingParameters</dfn> dictionary (defined in Section 5.2.4),
      the <dfn>RTCRtpEncodingParameters</dfn> dictionary (defined in Section 5.2.6),
      the <dfn>RTCRtpTransceiver</dfn> object including
      its <dfn>[[\Sender]]</dfn> and <dfn>[[\Receiver]]</dfn> internal slots (defined
      in Section 5.4), the <dfn>RTCRtpSender</dfn> object, including its
      <dfn>[[\SendEncodings]]</dfn> and <dfn>[[\LastReturnedParameters]]</dfn> internal slots (defined in Section 5.2),
      and the <dfn>RTCRtpReceiver</dfn> object (defined in Section 5.3).</p>
  </section>
  <section id="operational-model">
    <h2>Scope and Operational model</h2>
    <p>This specification extends the WebRTC specification [[!WEBRTC]] to
    enable user agents to support scalable video coding (SVC).</p>
    <p>For Scalable Video Coding (SVC), the terms single-session transmission
    (<dfn>SST</dfn>) and multi-session transmission (<dfn>MST</dfn>) are defined in
    [[RFC6190]]. This specification only supports <a>SST</a> but not <a>MST</a>. The
    term Single Real-time transport protocol stream Single Transport (<dfn>SRST</dfn>),
    defined in [[RFC7656]] Section 3.7, refers to Scalable Video Coding
    (<dfn>SVC</dfn>) implementations that transmit all layers within
    a single transport, using a single Real-time Transport Protocol (RTP) stream
    and synchronization source (SSRC). The term Multiple RTP stream Single Transport
    (<dfn>MRST</dfn>), also defined in [[RFC7656]] Section 3.7, refers to
    implementations that transmit all layers within a single transport, using
    multiple RTP streams with a distinct SSRC for each layer. This specification
    only supports <a>SRST</a> transport.</p>
    <p>Within this specification it is assumed that scalable video coding operation is primarily
    controlled utilizing the <code>addTransceiver</code> method of the <code><a>RTCPeerConnection</a></code>
    and the <code>setParameters</code> method of the <code><a>RTCRtpSender</a></code> object.</p>
    <p>The <code>addTransceiver</code> method establishes the <dfn>SVC envelope</dfn> which includes
    the maximum number of scalable video coding layers that can be sent, as well as the ordering of the <code>encodings</code>.
    While characteristics of individual scalable video coding layers can be modified using the <code>setParameters</code>
    method, the <a>SVC envelope</a> cannot be changed.  One of the implications of this model is that
    the <code>addTrack</code> method is not suitable for use with scalable video coding since this method
    does not take <code>sendEncodings</code> as an argument, and therefore will not result in
    configuration of an <code><a>RTCRtpTransceiver</a></code> with an <a>SVC envelope</a>.</p>
    <p>Note that the inability to modify the <a>SVC envelope</a> using <code>setParameters</code> does not restrict
    application flexibility in controlling the number of scalable video coding layers that are sent, or the characteristics of
    those layers. For example, using <code>setParameters</code> individual SVC layers can be made inactive by setting the <code>active</code>
    attribute to <code>false</code>, or reactivated by setting the <code>active</code>
    attribute to <code>true</code>.</p> 
    <p>This specification does not define how to configure <code>createOffer</code> to
    receive scalable video coding. Codecs such as VP8, VP9 or AV1 require a compliant decoder
    to be able to decode anything that an encoder can send so that configuration is not required
    to enable an <code><a>RTCRtpReceiver</a></code> to decode scalable video coding.</p> 
  </section>
  <section id="rtpdictionaries">
    <h2>Dictionary extensions</h2>
    <section id="rtcrtpencodingparameters">
      <h3>RTCRtpEncodingParameters Dictionary Extensions</h3>
      <div>
        <pre class="idl">partial dictionary RTCRtpEncodingParameters {
             double              scaleFramerateDownBy;
             DOMString           encodingId;
             sequence&lt;DOMString&gt; dependencyEncodingIds;
};</pre>
        <section>
          <h2>Dictionary <a class="idlType">RTCRtpEncodingParameters</a> Members</h2>
          <dl data-link-for="RTCRtpEncodingParameters" data-dfn-for=
          "RTCRtpEncodingParameters" class="dictionary-members">
            <dt><dfn data-idl><code>scaleFramerateDownBy</code></dfn> of type <span class=
            "idlMemberType"><a>double</a></span></dt>
            <dd>
              <p>Inverse of the input framerate fraction to be encoded. Example: 1.0 =
              full framerate, 2.0 = one half of the full framerate. For scalable video
              coding, <code>scaleFramerateDownBy</code> refers to the inverse of the aggregate
              fraction of input framerate achieved by this layer when combined with all
              dependent layers.</p>
            </dd>
            <dt><dfn data-idl><code>encodingId</code></dfn> of type <span class=
            "idlMemberType"><a>DOMString</a></span></dt>
            <dd>
              <p>An identifier for the encoding object. This identifier should be unique
              within the scope of the localized sequence of
              <code><a>RTCRtpEncodingParameters</a></code> for any given
              <code><a>RTCRtpParameters</a></code> object. Values MUST be composed only
              of alphanumeric characters (a-z, A-Z, 0-9) up to a maximum
              of 16 characters.</p>
            </dd>
            <dt><dfn data-idl><code>dependencyEncodingIds</code></dfn> of type <span class=
            "idlMemberType">sequence&lt;<a>DOMString</a>&gt;</span></dt>
            <dd>
              <p>The <code><a>encodingId</a></code>s on which this layer depends. Within
              this specification <code><a>encodingId</a></code>s are permitted only
              within the same <code><a>RTCRtpEncodingParameters</a></code> sequence. In
              the future if <a>MST</a> were to be supported, then if searching within an
              <code>encodings[]</code> sequence did not produce a match, then a global
              search would be carried out. In order to send scalable video coding
              (<a>SVC</a>), both the <code>encodingId</code> and
              <code>dependencyEncodingIds</code> are required.</p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    </section>
    <section id="rtcrtpencodingparameters-example*">
      <h3>Examples</h3>
        <section class="informative" id="rtcrtpencodingtemporal-example*">
          <h4>Temporal Scalability</h4>
          <pre class="example highlight">
// Example of 3-layer temporal scalability encoding with base framerate
// one quarter of the input, and ehancement layers providing one half
// and all of the input framerate
var encodings = [
  {encodingId: 'T0', scaleFramerateDownBy: 4.0},
  {encodingId: 'T1', scaleFramerateDownBy: 2.0, dependencyEncodingIds: ['T0']},
  {encodingId: 'T2', dependencyEncodingIds: ['T0', 'T1']} 
];

// Example of 3-layer temporal scalability with all but the base layer disabled
var encodings = [
  {encodingId: 'T0', scaleFramerateDownBy: 4.0, active: true}, 
  {encodingId: 'T1', scaleFramerateDownBy: 2.0, dependencyEncodingIds: ['T0'], active: false},
  {encodingId: 'T2', dependencyEncodingIds: ['T0', 'T1'], active: false}
];
</pre>
          <p>Below is a representation of a 3-layer temporal scalability encoding. In the
          diagram, I0 is the base layer I-frame, and P0 represents base-layer P-frames.
          P1 represents the first temporal enhancement layer, and P2 represents the
          second temporal enhancement layer.</p>
          <figure>
            <img alt="3-layer temporal scalability encoding" src=
            "images/3-layer-temporal.svg" style="width:75%">
            <figcaption>
              3-layer temporal scalability encoding
            </figcaption>
          </figure>
        </section>
        <section class="informative" id="rtcrtpencodingspatialsim-example*">
          <h4>Spatial Simulcast with Temporal Scalability</h4>
          <pre class="example highlight">
// Example of 2-layer spatial simulcast combined with 2-layer temporal scalability
// Low resolution base layer has half the input framerate, half the input resolution
// High resolution base layer has half the input framerate, full resolution
// Temporal enhancement layers have full input framerate
var encodings = [
  {encodingId: 'H0', scaleResolutionDownBy: 2.0, scaleFramerateDownBy: 2.0},
  {encodingId: 'F0', scaleFramerateDownBy: 2.0}, 
  {encodingId: 'H1', scaleResolutionDownBy: 2.0, dependencyEncodingIds: ['H0']},
  {encodingId: 'F1', dependencyEncodingIds: ['F0']}
];
                    </pre>
          <p>Below is a representation of 2-layer temporal scalability combined with
          2-layer spatial simulcast. Solid arrows represent temporal prediction. In the
          diagram, I0 is the base-layer I-frame, and P0 represents base-layer P-frames.
          EI0 is an enhanced resolution base-layer I-frame, and EP0 represents P-frames
          within the enhanced resolution base layer. P1 represents the first temporal
          enhancement layer, and EP1 represents a temporal enhancement to the enhanced
          resolution simulcast base-layer.</p>
          <figure>
            <img alt="2-layer spatial simulcast and temporal scalability encoding" src=
            "images/2-layer-spatialsim-temporal.svg" style="width:75%">
            <figcaption>
              2-layer spatial simulcast and temporal scalability encoding
            </figcaption>
          </figure>
        </section>
        <section class="informative" id="rtcrtpencodingspatialscal-example*">
          <h4>Spatial Scalability</h4>
          <pre class="example highlight">
// Example of 3-layer spatial scalability encoding with the base layer having
// one quarter input resolution and enhancement layers yielding one half and
// full resolution
var encodings = [
  {encodingId: 'q', scaleResolutionDownBy: 4.0},
  {encodingId: 'h', scaleResolutionDownBy: 2.0, dependencyEncodingIds: ['q']},
  {encodingId: 'f', dependencyEncodingIds: [['q', 'h']}
]

// Example of 3-layer spatial scalability with all but the base layer disabled
var encodings = [
  {encodingId: 'q', scaleResolutionDownBy: 4.0, active: true},
  {encodingId: 'h', scaleResolutionDownBy: 2.0, active: false},
  {encodingId: 'f', active: false}
]

// Example of 2-layer spatial scalability combined with 2-layer temporal scalability
// Base layer has one half input framerate and half resolution
// Temporal enhancement layer has full input framerate, half resolution
// Spatial enhancement to the base layer has half input framerate, full resolution
// Temporal enhancement to the spatial enhancement layer has full framerate and resolution
var encodings = [
  {encodingId: 'H0', scaleResolutionDownBy: 2.0, scaleFramerateDownBy: 2.0}, 
  {encodingId: 'H1', scaleResolutionDownBy: 2.0, dependencyEncodingIds: ['H0']},
  {encodingId: 'F0', scaleFramerateDownBy: 2.0, dependencyEncodingIds: ['H0']},
  {encodingId: 'F1', dependencyEnodingIds: ['F0', 'H1']}
];
                    </pre>
          <p>Below is a representation of 2-layer temporal scalability combined with
          2-layer spatial scalability. Solid arrows represent temporal prediction and
          dashed arrows represent inter-layer prediction. In the diagram, I0 is the
          base-layer I-frame, and EI0 is an intra spatial enhancement. P0 represents
          base-layer P-frames, and P1 represents the first temporal enhancement layer.
          EP0 represents a resolution enhancement to the base-layer P frames, and EP1
          represents a resolution enhancement to the second temporal layer P-frames.</p>
          <figure>
            <img alt="2-layer spatial and temporal scalability encoding" src=
            "images/2-layer-spatial-temporal.svg" style="width:75%">
            <figcaption>
              2-layer spatial and temporal scalability encoding
            </figcaption>
          </figure>
        </section>
   </section>
   </section>
    <section id="privacy-security">
    <h2>Privacy and Security Considerations</h2>
    <p>This section is non-normative; it specifies no new behaviour, but
    instead summarizes information already present in other parts of the
    specification. The overall security considerations of the
    APIs and protocols used in WebRTC are described in
    [[RTCWEB-SECURITY-ARCH]].</p>
    <section>
      <h2>Impact on same origin policy</h2>
      <p>This API enables data to be communicated between
      browsers and other devices, including other browsers.</p>
      <p>This means that data can be shared between applications
      running in different browsers, or between an application running in the
      same browser and something that is not a browser.  This is an extension
      to the Web model which has had barriers against sending data
      between entities with different origins.</p>
      <p>This specification provides no user prompts or chrome indicators
      for communication; it assumes that once the Web page has been allowed to
      access data, it is free to share that data with other entities as it
      chooses.</p>
    </section>
    <section>
      <h2>Impact on local network</h2>
      <p>Since the browser is an active platform executing in a trusted network
      environment (inside the firewall), it is important to limit the damage
      that the browser can do to other elements on the local network, and it is
      important to protect data from interception, manipulation and
      modification by untrusted participants.</p>
      <p>Mitigations include:</p>
      <ul>
        <li>An UA will always request permission from the correspondent UA to
        communicate using ICE. This ensures that the UA can only send to
        partners who you have shared credentials with.</li>
        <li>An UA will always request ongoing permission to continue sending
        using ICE consent [[!RFC7675]]. This enables a receiver to withdraw
        consent to receive.</li>
        <li>An UA will always encrypt data, with strong per-session keying.</li>
        <li>An UA will always use congestion control. This ensures that QUIC
        cannot be used to flood the network.</li>
      </ul>
      <p>These measures are specified in the relevant IETF documents.</p>
    </section>
    <section>
      <h2>Confidentiality of Communications</h2>
      <p>The fact that communication is taking place cannot be hidden from
      adversaries that can observe the network, so this has to be regarded as
      public information.</p>
    </section>
  </section>
 </section>
 <section id="examples*">
  <h2>Examples</h2>
   <p>This section describes how to utilize encoding parameters in various scenarios, as
   well as providing a code example.</p>
 </section>
 <section id="change-log*">
    <h2>Change Log</h2>
    <p>This section will be removed before publication.</p>
 </section>
 <section class="appendix">
    <h2>Acknowledgements</h2>
    <p>The editors wish to thank the Working Group chairs and Team Contact,
    Harald Alvestrand, Stefan H&aring;kansson, Bernard Aboba and Dominique
    Haza&euml;l-Massieux, for their support. Contributions to this
    specification were provided by Robin Raymond.</p>
    <p>Support for simulcast using the <code>RTCRtpEncodingParameters</code> dictionary
    was initially described in the <a href="https://www.w3.org/community/ortc/">W3C ORTC CG</a>,
    and has been adapted for use in this specification.</p>
 </section>
</body>
</html>
